{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import utils.for_tf as util\n",
    "from env.portfolio import *\n",
    "import utils.markets.indicators as ti\n",
    "from replay_buffer.replay_buffer import PrioritizedReplayBuffer as RBProportional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_raw = pd.read_csv('./data/price_data')\n",
    "prices_raw['Date'] = prices_raw['Date'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "volumes_raw = pd.read_csv('./data/volume_data')\n",
    "volumes_raw['Date'] = volumes_raw['Date'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "volumes_raw.set_index('Date',inplace=True)\n",
    "\n",
    "volumes_raw.columns = pd.MultiIndex.from_product([[i for i in volumes_raw.columns],['vol']])\n",
    "\n",
    "prices_raw.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = prices_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 8760\n"
     ]
    }
   ],
   "source": [
    "resampled_price = prices_raw.resample(\"1h\").ohlc().bfill()\n",
    "resampled_vol = volumes_raw.resample(\"1h\").asfreq().bfill()\n",
    "\n",
    "print(np.count_nonzero(np.isnan(resampled_price)),np.count_nonzero(np.isnan(resampled_vol)),len(resampled_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ind(df):\n",
    "    tmp = df\n",
    "    for a in [50,100,200]:\n",
    "        tmp = ti.moving_average(tmp,a)\n",
    "    tmp = ti.macd(tmp,12,26)\n",
    "    tmp = ti.relative_strength_index(tmp,10)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_price.columns = pd.MultiIndex.from_product([abbr,['Open','High','Low','Close']], names=('coins', 'feature'))\n",
    "#history_ind = {name : add_ind(pd.concat([resampled_price[name],resampled_vol[name]],axis=1)).dropna().values for name in abbr}\n",
    "history = {name : pd.concat([resampled_price[name],resampled_vol[name]],axis=1).values for name in abbr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 500, 5]\n"
     ]
    }
   ],
   "source": [
    "data_shape = [len(history),500,history['btc'].shape[1]]\n",
    "print(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, network, obs_dim,\n",
    "            num_actions, gamma=0.9, lam=0.95, reuse=tf.AUTO_REUSE):\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.t = 0\n",
    "        self.obss = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.next_values = []\n",
    "\n",
    "        act, train, update_old, backup_current = build_train(\n",
    "            network=network,\n",
    "            obs_dim=obs_dim,\n",
    "            num_actions=num_actions,\n",
    "            gamma=gamma,\n",
    "            reuse=reuse\n",
    "        )\n",
    "        self._act = act\n",
    "        self._train = train\n",
    "        self._update_old = update_old\n",
    "        self._backup_current = backup_current\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self._act([obs])[0][0]\n",
    "\n",
    "    def act_and_train(self, last_obs, last_action, last_value, reward, obs):\n",
    "        action, value = self._act([obs])\n",
    "        action = action[0]\n",
    "        value = value[0]\n",
    "        action = np.clip(action, -2, 2)\n",
    "\n",
    "        if last_obs is not None:\n",
    "            self._add_trajectory(\n",
    "                last_obs,\n",
    "                last_action,\n",
    "                reward,\n",
    "                last_value,\n",
    "                value\n",
    "            )\n",
    "\n",
    "        self.t += 1\n",
    "        return action, value\n",
    "\n",
    "    def train(self, obs, actions, returns, deltas):\n",
    "        self._backup_current()\n",
    "        loss, value_loss, ratio = self._train(obs, actions, returns, deltas)\n",
    "        print(loss, value_loss, ratio)\n",
    "        self._update_old()\n",
    "        return ratio\n",
    "\n",
    "    def stop_episode(self, last_obs, last_action, last_value, reward):\n",
    "        self._add_trajectory(\n",
    "            last_obs,\n",
    "            last_action,\n",
    "            reward,\n",
    "            last_value,\n",
    "            0\n",
    "        )\n",
    "\n",
    "    def _reset_trajectories(self):\n",
    "        self.obss = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.values = []\n",
    "        self.next_values = []\n",
    "\n",
    "    def _add_trajectory(self, obs, action, reward, value, next_value):\n",
    "        self.obss.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.next_values.append(next_value)\n",
    "\n",
    "    def get_training_data(self):\n",
    "        obss = list(self.obss)\n",
    "        actions = list(self.actions)\n",
    "        deltas = []\n",
    "        returns = []\n",
    "        V = 0\n",
    "        for i in reversed(range(len(self.obss))):\n",
    "            reward = self.rewards[i]\n",
    "            value = self.values[i]\n",
    "            next_value = self.next_values[i]\n",
    "            delta = reward + self.gamma * next_value - value\n",
    "            V = delta + self.lam * self.gamma * V\n",
    "            deltas.append(V)\n",
    "            returns.append(V + value)\n",
    "        deltas = np.array(list(reversed(deltas)), dtype=np.float32)\n",
    "        returns = np.array(list(reversed(returns)), dtype=np.float32)\n",
    "        # standardize advantages\n",
    "        deltas = (deltas - deltas.mean()) / (deltas.std() + 1e-5)\n",
    "        self._reset_trajectories()\n",
    "        return obss, actions, list(returns), list(deltas)\n",
    "\n",
    "    def sync_old(self):\n",
    "        self._update_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_train(network, obs_dim,\n",
    "            num_actions, gamma=1.0, epsilon=0.2, beta=0.01, scope='ppo', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        # input placeholders\n",
    "        obs_t_input = tf.placeholder(tf.float32, [None] + obs_dim, name='obs_t')\n",
    "        act_t_ph = tf.placeholder(tf.float32, [None, num_actions], name='action')\n",
    "        return_t_ph = tf.placeholder(tf.float32, [None, 1], name='return')\n",
    "        advantage_t_ph = tf.placeholder(tf.float32, [None, 1], name='advantage')\n",
    "\n",
    "        policy, value, dist = network(\n",
    "            obs_t_input,\n",
    "            num_actions,\n",
    "            scope='network',\n",
    "            reuse=reuse\n",
    "        )\n",
    "        network_func_vars = util.scope_vars(\n",
    "            util.absolute_scope_name('network'),\n",
    "            trainable_only=True\n",
    "        )\n",
    "\n",
    "        old_policy, old_value, old_dist = network(\n",
    "            obs_t_input,\n",
    "            num_actions,\n",
    "            scope='old_network',\n",
    "            reuse=reuse\n",
    "        )\n",
    "        old_network_func_vars = util.scope_vars(\n",
    "            util.absolute_scope_name('old_network'),\n",
    "            trainable_only=True\n",
    "        )\n",
    "\n",
    "        tmp_policy, tmp_value, tmp_dist = network(\n",
    "            obs_t_input,\n",
    "            num_actions,\n",
    "            scope='tmp_network',\n",
    "            reuse=reuse\n",
    "        )\n",
    "        tmp_network_func_vars = util.scope_vars(\n",
    "            util.absolute_scope_name('tmp_network'),\n",
    "            trainable_only=True\n",
    "        )\n",
    "\n",
    "        # clipped surrogate objective\n",
    "        cur_policy = dist.log_prob(act_t_ph + 1e-5)\n",
    "        old_policy = old_dist.log_prob(act_t_ph + 1e-5)\n",
    "        ratio = tf.exp(cur_policy - old_policy)\n",
    "        clipped_ratio = tf.clip_by_value(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "        surrogate = -tf.reduce_mean(\n",
    "            tf.minimum(ratio, clipped_ratio) * advantage_t_ph,\n",
    "            name='surrogate'\n",
    "        )\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            # value network loss\n",
    "            value_loss = tf.reduce_mean(tf.square(value - return_t_ph))\n",
    "\n",
    "            # entropy penalty for exploration\n",
    "            entropy = tf.reduce_mean(dist.entropy())\n",
    "            penalty = -beta * entropy\n",
    "\n",
    "            # total loss\n",
    "            loss = surrogate + value_loss + penalty\n",
    "\n",
    "        # optimize operations\n",
    "        optimizer = tf.train.AdamOptimizer(3 * 1e-4)\n",
    "        optimize_expr = optimizer.minimize(loss, var_list=network_func_vars)\n",
    "\n",
    "        # update old network operations\n",
    "        with tf.variable_scope('update_old_network'):\n",
    "            update_old_expr = []\n",
    "            sorted_tmp_vars = sorted(\n",
    "                tmp_network_func_vars,\n",
    "                key=lambda v: v.name\n",
    "            )\n",
    "            sorted_old_vars = sorted(\n",
    "                old_network_func_vars,\n",
    "                key=lambda v: v.name\n",
    "            )\n",
    "            for var_tmp, var_old in zip(sorted_tmp_vars, sorted_old_vars):\n",
    "                update_old_expr.append(var_old.assign(var_tmp))\n",
    "            update_old_expr = tf.group(*update_old_expr)\n",
    "\n",
    "        # update tmp network operations\n",
    "        with tf.variable_scope('update_tmp_network'):\n",
    "            update_tmp_expr = []\n",
    "            sorted_vars = sorted(network_func_vars, key=lambda v: v.name)\n",
    "            sorted_tmp_vars = sorted(\n",
    "                tmp_network_func_vars,\n",
    "                key=lambda v: v.name\n",
    "            )\n",
    "            for var, var_tmp in zip(sorted_vars, sorted_tmp_vars):\n",
    "                update_tmp_expr.append(var_tmp.assign(var))\n",
    "            update_tmp_expr = tf.group(*update_tmp_expr)\n",
    "\n",
    "        # action theano-style function\n",
    "        act = util.function(inputs=[obs_t_input], outputs=[policy, value])\n",
    "\n",
    "        # train theano-style function\n",
    "        train = util.function(\n",
    "            inputs=[\n",
    "                obs_t_input, act_t_ph, return_t_ph, advantage_t_ph\n",
    "            ],\n",
    "            outputs=[loss, value_loss, tf.reduce_mean(ratio)],\n",
    "            updates=[optimize_expr]\n",
    "        )\n",
    "\n",
    "        # update target theano-style function\n",
    "        update_old = util.function([], [], updates=[update_old_expr])\n",
    "        backup_current = util.function([], [], updates=[update_tmp_expr])\n",
    "\n",
    "        return act, train, update_old, backup_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_network(inpt, num_actions, scope='network', reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = inpt\n",
    "\n",
    "        conv_1 = tflearn.layers.conv_2d(inpt, 4, [1, 16],activation='leaky_relu', padding=\"valid\", bias=True,\n",
    "                                            name='conv_1')\n",
    "\n",
    "        conv_2 = tflearn.layers.conv_2d(conv_1, 8, [1,128-16+1],\n",
    "                                        activation='leaky_relu', \n",
    "                                        padding=\"valid\", bias=True,\n",
    "                                        regularizer='L2',\n",
    "                                        weight_decay=1e-5,\n",
    "                                        name='conv_2') \n",
    "\n",
    "        with tf.name_scope('Dense'):\n",
    "            net = tflearn.fully_connected(inpt, 2*10, name='1_dense')\n",
    "            net = tflearn.activations.leakyrelu(net,name='1_dense_LRelu')\n",
    "            net = tflearn.fully_connected(net, 10, name='2_dense')\n",
    "            net = tflearn.activations.leakyrelu(net,name='2_dense_LRelu')\n",
    "            net = tf.reshape(net,shape=[-1,10,1,1])\n",
    "        with tf.name_scope('merge'):\n",
    "            out = tf.concat([conv_2, net], axis = -1)\n",
    "            out = tf.layers.flatten(out)\n",
    "\n",
    "        # policy branch\n",
    "        mu = tf.layers.dense(\n",
    "            out,\n",
    "            num_actions,\n",
    "            kernel_initializer=tf.random_uniform_initializer(\n",
    "                minval=-3e-3,\n",
    "                maxval=3e-3\n",
    "            ),\n",
    "            name='mu3'\n",
    "        )\n",
    "        mu = tf.nn.tanh(mu + 1e-5)\n",
    "\n",
    "        sigma = tf.layers.dense(\n",
    "            out,\n",
    "            num_actions,\n",
    "            kernel_initializer=tf.random_uniform_initializer(\n",
    "                minval=-3e-3,\n",
    "                maxval=3e-3\n",
    "            ),\n",
    "            name='sigma3'\n",
    "        )\n",
    "        sigma = tf.nn.softplus(sigma + 1e-5)\n",
    "\n",
    "        dist = tf.distributions.Normal(mu, sigma)\n",
    "        policy = tf.nn.softmax(dist.sample())\n",
    "        \n",
    "        # value branch\n",
    "        \n",
    "        value = tf.layers.dense(\n",
    "            out,\n",
    "            1,\n",
    "            kernel_initializer=tf.random_uniform_initializer(\n",
    "                minval=-3e-3,\n",
    "                maxval=3e-3\n",
    "            ),\n",
    "            name='d3'\n",
    "        )\n",
    "    return policy, value, dist\n",
    "\n",
    "def make_network():\n",
    "    return lambda *args, **kwargs: _make_network(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summaries():\n",
    "    pol_sum = [tf.placeholder(tf.float64) for i in range(len(abbr))]\n",
    "    for ind, x in enumerate(abbr):\n",
    "        tf.summary.scalar(\"%s_policy\"%x, pol_sum[ind])\n",
    "\n",
    "    reward_summary = tf.placeholder(tf.float32, (), name='Reward')\n",
    "    tf.summary.scalar('Reward', reward_summary)\n",
    "\n",
    "    summary_vars = [reward_summary] + pol_sum\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "    \n",
    "    vardic = {v.name: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n",
    "    hist = []\n",
    "    for key, value in vardic.items():\n",
    "        hist.append(tf.summary.histogram(key[:-2], value))\n",
    "    hist_op = tf.summary.merge(hist)   \n",
    "\n",
    "    return hist_op, summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "'outdir': './logs/ppo/out',\n",
    "'logdir': './logs/ppo/log',\n",
    "'load': None,\n",
    "'final_steps': 10 ** 7,\n",
    "'batch': 64,\n",
    "'epoch': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./logs/ppo/\n",
    "!mkdir ./logs/ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode: 0, Step: 7001: Reward: 2.877212305978641\n",
      "-0.24707846 9.507516e-05 0.9996441\n",
      "-0.38075662 9.2216585e-05 1.0034016\n",
      "-0.7227026 0.00047442105 1.0041225\n",
      "-0.3937245 0.0009817599 1.0049675\n",
      "-0.2966677 0.0013893408 1.0064046\n",
      "-0.2883636 0.0012458336 1.0077386\n",
      "-0.24225627 0.001103835 1.0092189\n",
      "-0.2736369 0.0009131008 1.0107764\n",
      "-0.23813507 0.00078166486 1.0125216\n",
      "-0.27875057 0.00073807716 1.0142123\n",
      "-0.25138706 0.0008039543 1.0159403\n",
      "-0.23959762 0.0008694541 1.0176742\n",
      "-0.2694517 0.00080885115 1.0193305\n",
      "-0.23108041 0.0006801973 1.0214344\n",
      "-0.20063347 0.0006311163 1.0242211\n",
      "-0.13394165 0.00079379434 1.027716\n",
      "-0.12428826 0.00078722305 1.0306537\n",
      "-0.094245136 0.0006747592 1.03431\n",
      "-0.09336728 0.0005894797 1.0377527\n",
      "-0.067096904 0.00067417894 1.0419908\n",
      "-0.033223186 0.00070248486 1.0454032\n",
      "-0.018422121 0.00060780515 1.0454453\n",
      "0.024114605 0.0006093844 1.0425113\n",
      "-0.012908362 0.000683739 1.0293897\n",
      "-0.009974865 0.0006349534 1.0230278\n",
      "-0.009629082 0.00056836137 1.0152395\n",
      "-0.00043575442 0.000607197 1.0064635\n",
      "0.010151038 0.00059138436 0.9947527\n",
      "0.010923311 0.0005283687 0.97999734\n",
      "0.004655486 0.0005384309 0.96542734\n",
      "-0.010726808 0.00054335553 0.9526535\n",
      "-0.016046355 0.0004970776 0.94624305\n",
      "-0.021925896 0.00049477624 0.94612736\n",
      "-0.020488864 0.0004999873 0.95620364\n",
      "-0.013577836 0.00046372894 0.96864\n",
      "-0.014510959 0.0004577028 0.9738652\n",
      "-0.02241074 0.0004666215 0.9826387\n",
      "-0.023559717 0.0004445864 1.0028386\n",
      "-0.024185544 0.00043375848 1.0209553\n",
      "-0.01911409 0.0004331442 1.0326421\n",
      "-0.015180435 0.00042583662 1.0243065\n",
      "0.004230655 0.0004349846 1.0223969\n",
      "-0.008289756 0.0004353016 1.0018257\n",
      "0.013623416 0.0004391642 1.0002235\n",
      "0.019111881 0.00043026826 0.98012704\n",
      "0.03108503 0.0004955149 0.95704114\n",
      "0.02611095 0.00050355645 0.9315713\n",
      "0.023738004 0.0004880999 0.9039081\n",
      "0.021582361 0.00048546528 0.8664269\n",
      "0.01355505 0.00048031702 0.8153848\n",
      "Episode: 1, Step: 14002: Reward: 2.715715971600463\n",
      "-0.2025267 0.000102778344 0.7948528\n",
      "-0.5666646 0.0001522769 1.1103324\n",
      "-0.77157456 0.00021550251 1.1530455\n",
      "-0.3820007 0.00095495814 1.1180825\n",
      "-0.326006 0.0012491499 1.108729\n",
      "-0.37756452 0.00093579566 1.0785834\n",
      "-0.31805775 0.0008845667 1.0553913\n",
      "-0.3182308 0.0008538928 1.0398841\n",
      "-0.26516765 0.0007271709 1.0301672\n",
      "-0.28801376 0.0006107148 1.0244933\n",
      "-0.25549474 0.0006062701 1.0232697\n",
      "-0.2455216 0.0007057419 1.0241275\n",
      "-0.28607425 0.0006725809 1.0264474\n",
      "-0.2564127 0.00062735996 1.0312394\n",
      "-0.22469716 0.0005823455 1.0360371\n",
      "-0.14131358 0.00062868034 1.0401525\n",
      "-0.13934761 0.00063632795 1.0401272\n",
      "-0.11186789 0.0006307377 1.0403308\n",
      "-0.110539384 0.00058000674 1.0392207\n",
      "-0.087156825 0.0005563259 1.0387285\n",
      "-0.045912065 0.00061018235 1.0377978\n",
      "-0.024291964 0.00061793724 1.0330486\n",
      "0.02185151 0.00060415344 1.0259172\n",
      "-0.005782987 0.0006294485 1.0086731\n",
      "-0.0024649852 0.00064328633 0.9957009\n",
      "-0.011870964 0.0005984845 0.9821198\n",
      "-0.0029349285 0.0005922538 0.9715583\n",
      "0.005944669 0.00061273476 0.9597581\n",
      "0.003141108 0.0005758217 0.9469937\n",
      "-0.0030498472 0.0005524684 0.93886304\n",
      "-0.021396551 0.0005770527 0.9365923\n",
      "-0.027856415 0.0005547464 0.9476977\n",
      "-0.03051072 0.000528541 0.96479344\n",
      "-0.03368676 0.0005371465 0.98304874\n",
      "-0.028157705 0.00052270625 1.0001369\n",
      "-0.030924335 0.00049854146 1.0170931\n",
      "-0.04176523 0.0005109237 1.0655907\n",
      "-0.043868113 0.00050412875 1.0657135\n",
      "-0.051617548 0.0004787975 1.0656414\n",
      "-0.0549347 0.0004767673 1.0645528\n",
      "-0.048959903 0.0004766134 1.0665634\n",
      "-0.025985777 0.0004882587 1.0643393\n",
      "-0.033140883 0.0004857012 1.0608009\n",
      "-0.016578248 0.00049820106 1.0638202\n",
      "-0.010116374 0.00048859627 1.0655898\n",
      "0.005773351 0.00054240535 1.066421\n",
      "-0.0011921874 0.0005482232 1.066145\n",
      "-0.00070253527 0.0005355478 1.065522\n",
      "0.00030754204 0.0005287738 1.0635512\n",
      "-0.0050728978 0.0005260266 1.0557048\n",
      "Episode: 2, Step: 21003: Reward: 2.564254178596653\n",
      "-0.3110481 0.00013253425 1.021731\n",
      "-0.5199227 0.00011274276 1.0471677\n",
      "-0.83184326 0.000282057 1.0876162\n",
      "-0.42565134 0.0010015684 1.1321507\n",
      "-0.36622033 0.0012012171 1.1550325\n",
      "-0.40938467 0.00085470866 1.1665496\n",
      "-0.3531697 0.0010669257 1.2026997\n",
      "-0.35310465 0.00077700126 1.2125473\n",
      "-0.27914426 0.00071768096 1.1835331\n",
      "-0.2830544 0.0008035799 1.3908502\n",
      "-0.24415123 0.00052906084 3.4704366\n",
      "-0.2003439 0.0008514251 87.0478\n",
      "-0.20081645 0.0005877653 6926.859\n",
      "-0.16310868 0.0008090187 7972.0723\n",
      "-0.14466211 0.00062619726 inf\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Nan in summary histogram for: ppo/network/conv_2/W_1\n\t [[Node: ppo/network/conv_2/W_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/network/conv_2/W_1/tag, ppo/network/conv_2/W/read)]]\n\nCaused by op 'ppo/network/conv_2/W_1', defined at:\n  File \"/Users/gidman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/gidman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-69-ef77825fe786>\", line 27, in <module>\n    hist_op, summary_ops, summary_vars = build_summaries()\n  File \"<ipython-input-59-14bc663dd031>\", line 15, in build_summaries\n    hist.append(tf.summary.histogram(key[:-2], value))\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/summary/summary.py\", line 203, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 283, in histogram_summary\n    \"HistogramSummary\", tag=tag, values=values, name=name)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: ppo/network/conv_2/W_1\n\t [[Node: ppo/network/conv_2/W_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/network/conv_2/W_1/tag, ppo/network/conv_2/W/read)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: ppo/network/conv_2/W_1\n\t [[Node: ppo/network/conv_2/W_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/network/conv_2/W_1/tag, ppo/network/conv_2/W/read)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ef77825fe786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n\u001b[1;32m    119\u001b[0m             \u001b[0mk\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0msummary_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Nan in summary histogram for: ppo/network/conv_2/W_1\n\t [[Node: ppo/network/conv_2/W_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/network/conv_2/W_1/tag, ppo/network/conv_2/W/read)]]\n\nCaused by op 'ppo/network/conv_2/W_1', defined at:\n  File \"/Users/gidman/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/gidman/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-69-ef77825fe786>\", line 27, in <module>\n    hist_op, summary_ops, summary_vars = build_summaries()\n  File \"<ipython-input-59-14bc663dd031>\", line 15, in build_summaries\n    hist.append(tf.summary.histogram(key[:-2], value))\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/summary/summary.py\", line 203, in histogram\n    tag=tag, values=values, name=scope)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 283, in histogram_summary\n    \"HistogramSummary\", tag=tag, values=values, name=name)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/gidman/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Nan in summary histogram for: ppo/network/conv_2/W_1\n\t [[Node: ppo/network/conv_2/W_1 = HistogramSummary[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ppo/network/conv_2/W_1/tag, ppo/network/conv_2/W/read)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "env = PortfolioEnv(np.array(list((history.values()))),\\\n",
    "                       abbr,steps=7000, \\\n",
    "                       window_length=128, \n",
    "                       trading_cost=0,\\\n",
    "                       sample_start_date='2017-05-10 19:00:00',\\\n",
    "                       start_idx=128)\n",
    "\n",
    "obs_dim = list(env.observation_space.shape)\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "network = make_network()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    agent = Agent(network, obs_dim, n_actions)\n",
    "\n",
    "    util.initialize()\n",
    "    #agent.sync_old()\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if args['load'] is not None:\n",
    "        saver.restore(sess, args['load'])\n",
    "        \n",
    "    hist_op, summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    global_step = 0\n",
    "    episode = 0\n",
    "    while True:\n",
    "        local_step = 0\n",
    "\n",
    "        while True:\n",
    "            training_data = []\n",
    "            sum_of_reward = 0\n",
    "            reward = 0\n",
    "            obs = env.reset()[0]\n",
    "            last_obs = None\n",
    "            last_action = None\n",
    "            last_value = None\n",
    "            done = False\n",
    "            train_writer = tf.summary.FileWriter('%s/train/episode_%s' % (args['logdir'], episode), sess.graph)\n",
    "            while not done:\n",
    "                action, value = agent.act_and_train(\n",
    "                        last_obs, last_action, last_value, reward,  obs)\n",
    "\n",
    "                last_obs = obs\n",
    "                last_action = action\n",
    "                last_value = value\n",
    "                obs, reward, done, info, y = env.step(action)\n",
    "\n",
    "                sum_of_reward += reward\n",
    "                global_step += 1\n",
    "                local_step += 1\n",
    "\n",
    "                # save model\n",
    "                if global_step % 20 ** 6 == 0:\n",
    "                    path = os.path.join(args['outdir'],\n",
    "                            '{}/model.ckpt'.format(global_step))\n",
    "                    saver.save(sess, path)\n",
    "\n",
    "                # summary\n",
    "                if (global_step % 7001) % 10 == 1:\n",
    "                    feed_dict={\n",
    "                        summary_vars[0]: sum_of_reward\n",
    "                    }\n",
    "                    pol_dict = {}\n",
    "                    for k in range(len(action)):\n",
    "                        pol_dict[summary_vars[1+k]] = action[k]\n",
    "                    feed_dict.update(pol_dict)\n",
    "\n",
    "                    summary_str = sess.run(summary_ops, feed_dict=feed_dict) \n",
    "                    train_writer.add_summary(summary_str, global_step % 7001)\n",
    "                    train_writer.flush()\n",
    "                    \n",
    "                #end of episode    \n",
    "                if done:\n",
    "                    agent.stop_episode(\n",
    "                            last_obs, last_action, last_value, reward)\n",
    "                    print(\n",
    "                        'Episode: {}, Step: {}: Reward: {}'.format(\n",
    "                        episode,\n",
    "                        global_step,\n",
    "                        sum_of_reward\n",
    "                    ))\n",
    "                    episode += 1\n",
    "                    break\n",
    "                    \n",
    "            # append data for training\n",
    "            training_data.append(agent.get_training_data())\n",
    "\n",
    "            if local_step > 2048:\n",
    "                break\n",
    "\n",
    "        # train network\n",
    "        obs = []\n",
    "        actions = []\n",
    "        returns = []\n",
    "        deltas = []\n",
    "        k = 0\n",
    "        for o, a, r, d in training_data:\n",
    "            obs.extend(o)\n",
    "            actions.extend(a)\n",
    "            returns.extend(r)\n",
    "            deltas.extend(d)\n",
    "        for epoch in range(50):       \n",
    "            indices = range(k,(k+1)*args['batch'])\n",
    "            sampled_obs = np.array(obs)[indices]\n",
    "            sampled_actions = np.array(actions)[indices]\n",
    "            sampled_returns = np.array(returns)[indices]\n",
    "            sampled_deltas = np.array(deltas)[indices]\n",
    "            ratio = agent.train(\n",
    "                sampled_obs,\n",
    "                sampled_actions,\n",
    "                sampled_returns,\n",
    "                sampled_deltas\n",
    "            )\n",
    "            k += 1\n",
    "            summary_hist = sess.run(hist_op) \n",
    "            train_writer.add_summary(summary_hist, epoch)\n",
    "            train_writer.flush()\n",
    "\n",
    "        if args['final_steps'] < global_step:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
