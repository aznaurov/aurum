{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gidman/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import itertools\n",
    "import keras\n",
    "\n",
    "import os\n",
    "\n",
    "from env.portfolio import *\n",
    "import utils.markets.indicators as ti\n",
    "from replay_buffer.replay_buffer import PrioritizedReplayBuffer as RBProportional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_raw = pd.read_csv('./data/price_data')\n",
    "prices_raw['Date'] = prices_raw['Date'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "volumes_raw = pd.read_csv('./data/volume_data')\n",
    "volumes_raw['Date'] = volumes_raw['Date'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))\n",
    "volumes_raw.set_index('Date',inplace=True)\n",
    "\n",
    "volumes_raw.columns = pd.MultiIndex.from_product([[i for i in volumes_raw.columns],['vol']])\n",
    "\n",
    "prices_raw.set_index('Date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbr = prices_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 8760\n"
     ]
    }
   ],
   "source": [
    "resampled_price = prices_raw.resample(\"1h\").ohlc().bfill()\n",
    "resampled_vol = volumes_raw.resample(\"1h\").asfreq().bfill()\n",
    "\n",
    "print(np.count_nonzero(np.isnan(resampled_price)),np.count_nonzero(np.isnan(resampled_vol)),len(resampled_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ind(df):\n",
    "    tmp = df\n",
    "    for a in [50,100,200]:\n",
    "        tmp = ti.moving_average(tmp,a)\n",
    "    tmp = ti.macd(tmp,12,26)\n",
    "    tmp = ti.relative_strength_index(tmp,10)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_price.columns = pd.MultiIndex.from_product([abbr,['Open','High','Low','Close']], names=('coins', 'feature'))\n",
    "history_ind = {name : add_ind(pd.concat([resampled_price[name],resampled_vol[name]],axis=1)).dropna().values for name in abbr}\n",
    "history = {name : pd.concat([resampled_price[name],resampled_vol[name]],axis=1).values for name in abbr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 500, 5]\n"
     ]
    }
   ],
   "source": [
    "data_shape = [len(history),500,history['btc'].shape[1]]\n",
    "print(data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed=123):\n",
    "        \"\"\"\n",
    "        The right side of the deque contains the most recent experiences \n",
    "        \"\"\"\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, s2, t, y):\n",
    "        experience = (s, a, r, s2, t, y)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = deque(itertools.islice(self.buffer, 0, self.count))\n",
    "        else:\n",
    "            tmp = int(random.uniform(0,self.count-batch_size))\n",
    "            batch = deque(itertools.islice(self.buffer, self.count-batch_size, self.count))\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        s2_batch = np.array([_[3] for _ in batch])\n",
    "        t_batch = np.array([_[4] for _ in batch]) \n",
    "        y_batch = np.array([_[5] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, s2_batch, t_batch, y_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size, params):\n",
    "        self.params = params\n",
    "        self.sess = sess\n",
    "        self.s_dim = list(state_dim)\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        with tf.variable_scope('Actor_main_net'):\n",
    "            self.inputs, self.out = self.create_actor_network()\n",
    "            self.network_params = tf.trainable_variables()\n",
    "        \n",
    "        # Target Network\n",
    "        with tf.variable_scope('Actor_traget_net'):\n",
    "            self.target_inputs, self.target_out = self.create_actor_network()\n",
    "            self.target_network_params = tf.trainable_variables()[\n",
    "                len(self.network_params):]\n",
    "        \n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Combine the gradients here\n",
    "        with tf.variable_scope(\"Actor_grads\"):\n",
    "            # This gradient will be provided by the critic network\n",
    "            self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "            self.unnormalized_actor_gradients = tf.gradients(\n",
    "                self.out, self.network_params, -self.action_gradient)\n",
    "            self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "            \n",
    "        \n",
    "        self.prices = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "        with tf.name_scope('loss_pretrain'):   \n",
    "            self.loss_obj = -tf.reduce_mean(tf.log(tf.reduce_sum(self.out[1:,:] * self.prices[:-1,:], reduction_indices=[1]) - \\\n",
    "                        tf.reduce_sum(tf.abs(self.out[1:,:] - self.out[:-1,:])*self.params['comission_ratio'], reduction_indices=[1])))\n",
    "            \n",
    "            self.optimize_pretrain = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss_obj)\n",
    "            \n",
    "            \n",
    "\n",
    "        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=([None] + self.s_dim), name='ohlcv_input')\n",
    "        #prev_policy_input = tf.placeholder(shape=[None,size[0],1,1], dtype=tf.float32, name='prev_policy_input')\n",
    "\n",
    "        with tf.name_scope('CNN'):\n",
    "            conv_1 = tflearn.layers.conv_2d(inputs, 4, [1, self.params['conv_width']],activation='leaky_relu', padding=\"valid\", bias=True,\n",
    "                                                name='conv_1')\n",
    "            if self.params['bn']:\n",
    "                conv_1 = tflearn.layers.normalization.batch_normalization(conv_1,name='conv_1_BN')\n",
    "            conv_2 = tflearn.layers.conv_2d(conv_1, 8, [1,self.s_dim[1]-self.params['conv_width']+1],\n",
    "                                            activation='leaky_relu', \n",
    "                                            padding=\"valid\", bias=True,\n",
    "                                            regularizer='L2',\n",
    "                                            weight_decay=self.params['weight_decay_1'],\n",
    "                                            name='conv_2') \n",
    "            if self.params['bn']:\n",
    "                conv_2 = tflearn.layers.normalization.batch_normalization(conv_2,name='conv_2_BN')\n",
    "        with tf.name_scope('Dense'):\n",
    "            net = tflearn.fully_connected(inputs, 2*self.s_dim[0], name='1_dense')\n",
    "            if self.params['bn']:\n",
    "                net = tflearn.layers.normalization.batch_normalization(net,name='1_dense_BN')\n",
    "            net = tflearn.activations.leakyrelu(net,name='1_dense_LRelu')\n",
    "            net = tflearn.fully_connected(net, self.s_dim[0], name='2_dense')\n",
    "            if self.params['bn']:\n",
    "                net = tflearn.layers.normalization.batch_normalization(net,name='2_dense_BN')\n",
    "            net = tflearn.activations.leakyrelu(net,name='2_dense_LRelu')\n",
    "            net = tf.reshape(net,shape=[-1,self.s_dim[0],1,1])\n",
    "        with tf.name_scope('merge'):\n",
    "            concat_1 = keras.layers.concatenate([conv_2, net])\n",
    "            conv_3 = tflearn.layers.conv_2d(concat_1, 1, [1, 1], \n",
    "                                            padding=\"valid\", bias=False,\n",
    "                                            regularizer='L2',\n",
    "                                            weight_decay=self.params['weight_decay_2'],\n",
    "                                            name='voting')\n",
    "            if self.params['bn']:\n",
    "                conv_3 = tflearn.layers.normalization.batch_normalization(conv_3,name='voting_BN')\n",
    "        out = tf.nn.softmax(conv_3,axis=1,name='out')[:,:,0,0]\n",
    "        return inputs, out\n",
    "    \n",
    "    def create_actor_network_2(self):\n",
    "        inputs = tflearn.input_data(shape=[None]+ self.s_dim)\n",
    "        net = tflearn.fully_connected(inputs, 100, name='dense_input',bias=False)\n",
    "        if self.params['bn']:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net,name='input_BN')\n",
    "        net = tflearn.activations.leakyrelu(net)\n",
    "        net = tflearn.fully_connected(net, 50,name='dense',bias=False)\n",
    "        if self.params['bn']:\n",
    "            net = tflearn.layers.normalization.batch_normalization(net,name='dense_BN')\n",
    "        net = tflearn.activations.leakyrelu(net)\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='softmax', name='out',bias=False)\n",
    "\n",
    "        return inputs, out\n",
    "    \n",
    "    def create_actor_network_1(self):\n",
    "        inputs = tflearn.input_data(shape=([None] + self.s_dim), name='ohlcv_input')\n",
    "        with tf.name_scope('CNN'):\n",
    "            conv_1 = tflearn.layers.conv_2d(inputs, 8, \n",
    "                                            [1, self.params['conv_width']],\n",
    "                                            activation='leaky_relu', padding=\"valid\", \n",
    "                                            bias=False,\n",
    "                                            regularizer='L2',\n",
    "                                            weight_decay=self.params['weight_decay_1'],\n",
    "                                            name='conv_1')\n",
    "            if self.params['dropout'] < 1:\n",
    "                conv_1 = tflearn.layers.dropout(conv_1, self.params['dropout'])\n",
    "            if self.params['bn']:\n",
    "                conv_1 = tflearn.layers.normalization.batch_normalization(conv_1,name='conv_1_BN')\n",
    "            conv_2 = tflearn.layers.conv_2d(conv_1, 16, [1,self.s_dim[1]-self.params['conv_width']+1],\n",
    "                                            activation='leaky_relu', \n",
    "                                            padding=\"valid\", bias=False,\n",
    "                                            regularizer='L2',\n",
    "                                            weight_decay=self.params['weight_decay_2'],\n",
    "                                            name='conv_2')\n",
    "            if self.params['dropout'] < 1:\n",
    "                conv_2 = tflearn.layers.dropout(conv_2, self.params['dropout'])\n",
    "            if self.params['bn']:\n",
    "                conv_2 = tflearn.layers.normalization.batch_normalization(conv_2,name='conv_2_BN')\n",
    "        with tf.name_scope('Dense'):     \n",
    "            net = tflearn.flatten(conv_2)\n",
    "            net = tflearn.fully_connected(net, 32, name='dense_1')\n",
    "            net = tflearn.activations.leakyrelu(net)\n",
    "            if self.params['dropout'] < 1:\n",
    "                net = tflearn.layers.dropout(net, self.params['dropout'])\n",
    "            if self.params['bn']:\n",
    "                net = tflearn.layers.normalization.batch_normalization(net,name='dense_1_BN')\n",
    "            net = tflearn.fully_connected(net, 32, name='dense_2')\n",
    "            net = tflearn.activations.leakyrelu(net)\n",
    "            if self.params['dropout'] < 1:\n",
    "                net = tflearn.layers.dropout(net, self.params['dropout'])\n",
    "            if self.params['bn']:\n",
    "                net = tflearn.layers.normalization.batch_normalization(net,name='dense_2_BN')\n",
    "        out = tflearn.fully_connected(\n",
    "            net, self.a_dim, activation='softmax', name='out')\n",
    "\n",
    "        return inputs, out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            #self.prev_policy_input: prev_policy_input,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "        \n",
    "    def pretrain(self, inputs, prices):\n",
    "        self.sess.run(self.optimize_pretrain, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.prices: prices\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars, params):\n",
    "        self.params = params\n",
    "        self.sess = sess\n",
    "        self.s_dim = list(state_dim)\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        with tf.variable_scope('Critic_main_net'):\n",
    "            self.inputs, self.action, self.out = self.create_critic_network()\n",
    "            self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        with tf.variable_scope('Critic_traget_net'):\n",
    "            self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "            self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        if self.params['with_batch_weights']:\n",
    "            self.w = tf.placeholder(dtype=tf.float32,shape=[None, 1])\n",
    "            print(self.out)\n",
    "            self.out_w = tf.multiply(self.w,self.out)\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        \n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        '''\n",
    "         with tf.variable_scope('C_train'):\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "            # Get the gradient pairs (Tensor, Variable)\n",
    "            self.grads = tf.gradients(self.loss, self.network_params)\n",
    "            # Update the weights wrt to the gradient\n",
    "            self.optimize = self.optimizer.apply_gradients(self.grads)\n",
    "        '''\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        if self.params['with_batch_weights']:\n",
    "            self.action_grads = tf.gradients(self.out_w, self.action, name = \"critic_action_grads\" )\n",
    "        else:\n",
    "            self.action_grads = tf.gradients(self.out, self.action, name = \"critic_action_grads\" )\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=([None] + self.s_dim))\n",
    "        action = tflearn.input_data(shape=([None,self.a_dim]))\n",
    "        with tf.name_scope(\"dense\"):\n",
    "            net = tflearn.fully_connected(inputs, 128,name='input_dense')\n",
    "            if self.params['bn']:\n",
    "                net = tflearn.layers.normalization.batch_normalization(net,name='input_dense_BN')\n",
    "            net = tflearn.activations.leakyrelu(net,name='input_dense_LRelu')\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "        with tf.name_scope(\"merge\"):\n",
    "            t1 = tflearn.fully_connected(net, 32,name='merge_net')\n",
    "            t2 = tflearn.fully_connected(action, 32, name='merge_action')\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t1.b + t2.b, activation='leaky_relu',name='merge_LRelu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        out = tflearn.fully_connected(net, 1, name = 'out',bias=False)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value, weights = None):\n",
    "        if self.params['with_batch_weights']:\n",
    "            return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "                self.inputs: inputs,\n",
    "                self.action: action,\n",
    "                self.predicted_q_value: predicted_q_value,\n",
    "                self.w : weights\n",
    "            })\n",
    "        else:\n",
    "            return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "                self.inputs: inputs,\n",
    "                self.action: action,\n",
    "                self.predicted_q_value: predicted_q_value\n",
    "            })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action, weights=None):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action,\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions, weights = None):\n",
    "        if self.params['with_batch_weights']:\n",
    "            return self.sess.run(self.action_grads, feed_dict={\n",
    "                self.inputs: inputs,\n",
    "                self.action: actions,\n",
    "                self.w: weights\n",
    "            })\n",
    "        else:\n",
    "            return self.sess.run(self.action_grads, feed_dict={\n",
    "                self.inputs: inputs,\n",
    "                self.action: actions\n",
    "            })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise:\n",
    "    def __init__(self, mu, sigma=1, theta=0.0, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x =  self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'ActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar(\"Qmax_Value_ave\", episode_ave_max_q)\n",
    "    step_max_q = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar(\"Qmax_Value\", step_max_q)\n",
    "    pol_sum = [tf.placeholder(tf.float32) for i in range(len(abbr))]\n",
    "    for ind, x in enumerate(abbr):\n",
    "        tf.summary.scalar(\"%s_policy\"%x, pol_sum[ind])\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, step_max_q] + pol_sum\n",
    "    vardic = {v.name: v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)}\n",
    "    for key, value in vardic.items():\n",
    "        tf.summary.histogram(key[:-2], value)\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain(sess, env, args, actor, summary_ops, summary_vars):\n",
    "\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = OldReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "    \n",
    "    for i in range(int(args['max_episodes_pretrain'])):\n",
    "\n",
    "        s = env.reset()[0]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        \n",
    "        writer = tf.summary.FileWriter('%s/pretrain/episode_%s' % (args['summary_dir'], i), sess.graph)\n",
    "        \n",
    "        for j in range(int(args['max_episode_len'])):\n",
    "            ep_q = 0\n",
    "            a = actor.predict(np.reshape(s, ([1]+ actor.s_dim)))\n",
    "            s2, r, terminal, info , y = env.step(a[0])\n",
    "            replay_buffer.add(s, np.reshape(a, (actor.a_dim,)), r, s2, terminal, y)\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, s2_batch, t_batch, y_batch = replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                actor.pretrain(s_batch, y_batch)\n",
    "                \n",
    "            s = s2\n",
    "            ep_reward += r  \n",
    "            \n",
    "            if terminal or j%10==1 or j==0:\n",
    "                feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j+1),\n",
    "                    summary_vars[2]: ep_q\n",
    "                }\n",
    "                pol_dict = {}\n",
    "                for k in range(len(a[0])):\n",
    "                    pol_dict[summary_vars[3+k]] = a[0][k]\n",
    "                feed_dict.update(pol_dict)\n",
    "                summary_str = sess.run(summary_ops, feed_dict=feed_dict)\n",
    "                \n",
    "                writer.add_summary(summary_str, j)\n",
    "                writer.flush()\n",
    "                \n",
    "                if terminal:\n",
    "                    print('| Reward: {:.6f} | Episode: {:d} | Qmax: {:.4f} | j: {:d}'.format(ep_reward, \\\n",
    "                            i, (ep_ave_max_q / float(j)),j))\n",
    "                    break\n",
    "                    \n",
    "        replay_buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, env, args, actor, critic, actor_noise, summary_ops, summary_vars):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    #summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    if args['prioritised_replay']:\n",
    "        replay_buffer = RBProportional(int(args['buffer_size']),args['prioritised_replay_alpha'])\n",
    "    else:\n",
    "        replay_buffer = OldReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "    \n",
    "\n",
    "    for i in range(int(args['max_episodes'])):\n",
    "\n",
    "        s = env.reset()[0]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        \n",
    "        writer = tf.summary.FileWriter('%s/train/episode_%s' % (args['summary_dir'], i), sess.graph)\n",
    "\n",
    "        for j in range(int(args['max_episode_len'])):\n",
    "            ep_q = 0\n",
    "            if args['render_env']:\n",
    "                env.render(mode='ansi')\n",
    "\n",
    "            # Added exploration noise\n",
    "            noise = 0.1*actor_noise()\n",
    "            noise = noise-np.sum(noise)/len(noise)\n",
    "            a = actor.predict(np.reshape(s, ([1]+ actor.s_dim))) + noise\n",
    "            \n",
    "            s2, r, terminal, info, y = env.step(a[0])\n",
    "            \n",
    "            replay_buffer.add(s, np.reshape(a, (actor.a_dim,)), r,\n",
    "                              s2, terminal, y)\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if args['prioritised_replay']:\n",
    "                if len(replay_buffer._storage) > int(args['minibatch_size']):\n",
    "                    s_batch, a_batch, r_batch, s2_batch, t_batch, batch_weights, index = \\\n",
    "                         replay_buffer.sample(int(args['minibatch_size']),args['beta'])\n",
    "                    batch_weights = np.transpose([batch_weights])\n",
    "                    target_q = critic.predict_target(s2_batch, actor.predict_target(s2_batch), batch_weights)\n",
    "                    delta = np.abs(np.transpose([r_batch]) + \\\n",
    "                        critic.gamma * target_q - \\\n",
    "                        critic.predict(s_batch, a_batch))[:,0]\n",
    "\n",
    "                    replay_buffer.update_priorities(index, delta)\n",
    "                    \n",
    "                    y_i = []\n",
    "                    for k in range(int(args['minibatch_size'])):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "                    \n",
    "                     # Update the critic given the targets\n",
    "                    predicted_q_value, _ = critic.train(\n",
    "                        s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)),batch_weights)\n",
    "\n",
    "                    ep_q = np.amax(predicted_q_value)\n",
    "                    ep_ave_max_q += ep_q\n",
    "\n",
    "                    # Update the actor policy using the sampled gradient\n",
    "                    if i >= 0:\n",
    "                        a_outs = actor.predict(s_batch)\n",
    "                        grads = critic.action_gradients(s_batch, a_outs, batch_weights)\n",
    "                        actor.train(s_batch, grads[0])\n",
    "\n",
    "                    # Update target networks\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "            elif replay_buffer.size() > int(args['minibatch_size']):\n",
    "                    s_batch, a_batch, r_batch, s2_batch, t_batch, y_batch = \\\n",
    "                        replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                        s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                    y_i = []\n",
    "                    for k in range(int(args['minibatch_size'])):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                    # Update the critic given the targets\n",
    "                    predicted_q_value, _ = critic.train(\n",
    "                        s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                    ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                    # Update the actor policy using the sampled gradient\n",
    "                    if i > 3:\n",
    "                        a_outs = actor.predict(s_batch)\n",
    "                        grads = critic.action_gradients(s_batch, a_outs)\n",
    "                        actor.train(s_batch, grads[0])\n",
    "\n",
    "                    # Update target networks\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j%10==1 or j==0:\n",
    "                feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j+1),\n",
    "                    summary_vars[2]: ep_q\n",
    "                }\n",
    "                pol_dict = {}\n",
    "                for k in range(len(a[0])):\n",
    "                    pol_dict[summary_vars[3+k]] = a[0][k]\n",
    "                feed_dict.update(pol_dict)\n",
    "                \n",
    "                \n",
    "                summary_str = sess.run(summary_ops, feed_dict=feed_dict)\n",
    "                \n",
    "                writer.add_summary(summary_str, j)\n",
    "                writer.flush()\n",
    "                \n",
    "                if terminal:\n",
    "                    print('| Reward: {:.6f} | Episode: {:d} | Qmax: {:.4f} | j: {:d}'.format(ep_reward, \\\n",
    "                            i, (ep_ave_max_q / float(j)),j))\n",
    "                    break\n",
    "        \"\"\"\n",
    "         if i%100==0 and i!=0:\n",
    "            model_save_path=\"./tmp/ddpg/1/\"\n",
    "            if not os.path.exists(model_save_path):\n",
    "                os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            model_path = saver.save(sess, model_save_path+(\"%s\"%i))\n",
    "            print(\"Model saved in %s\" % model_path)\n",
    "        \"\"\"       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'random_seed' : 123,\n",
    "        'actor_lr' : 0.00005,\n",
    "        'critic_lr': 0.00025,\n",
    "        'minibatch_size' : 32,\n",
    "        'tau' : 0.001,\n",
    "        'buffer_size' : 100,        \n",
    "        'gamma' : 0.999,\n",
    "        'max_episodes' : 10000,\n",
    "        'max_episodes_pretrain' : 1,\n",
    "        'max_episode_len' : 7000,\n",
    "        'render_env' : False,\n",
    "        'use_gym_monitor' : False,\n",
    "        'monitor_dir' : \"./logs/mon/\",\n",
    "        'summary_dir' : './logs/ddpg/13',\n",
    "        'prioritised_replay': False,\n",
    "        'prioritised_replay_alpha': 1,\n",
    "        'beta': 1,\n",
    "        'comission_ratio': 0.0025\n",
    "       } \n",
    "\n",
    "actor_net_params = {\n",
    "    'conv_width' : 16,\n",
    "    'weight_decay_1' : 1e-5,\n",
    "    'weight_decay_2' : 1e-5,\n",
    "    'bn' : False,\n",
    "    'dropout': 0.5,\n",
    "    'comission_ratio': 0.0025\n",
    "}\n",
    "\n",
    "critic_net_params = {\n",
    "    'bn' : False,\n",
    "    'with_batch_weights': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./logs/ddpg/\n",
    "!mkdir ./logs/ddpg\n",
    "!mkdir ./logs/ddpg/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <PortfolioEnv instance>\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-e0e6265062d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#pretrain(session, env, args, actor, summary_ops, summary_vars)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_noise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_gym_monitor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-77fd97f4e9ff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, args, actor, critic, actor_noise, summary_ops, summary_vars)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Initialize target network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-31f4d72e89d4>\u001b[0m in \u001b[0;36mupdate_target_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_network_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_num_trainable_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    env = PortfolioEnv(np.array(list((history_ind.values()))),\\\n",
    "                       abbr,steps=7000, \\\n",
    "                       window_length=128, \n",
    "                       trading_cost=0,\\\n",
    "                       sample_start_date='2017-05-10 19:00:00',\\\n",
    "                       start_idx=128)\n",
    "    np.random.seed(int(args['random_seed']))\n",
    "    tf.set_random_seed(int(args['random_seed']))\n",
    "    env.seed(int(args['random_seed']))\n",
    "\n",
    "    state_dim = env.observation_space.shape\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = 1\n",
    "\n",
    "    actor = ActorNetwork(session, state_dim, action_dim, action_bound,\n",
    "                         float(args['actor_lr']), float(args['tau']),\n",
    "                         int(args['minibatch_size']),actor_net_params)\n",
    "\n",
    "    critic = CriticNetwork(session, state_dim, action_dim,\n",
    "                           float(args['critic_lr']), float(args['tau']),\n",
    "                           float(args['gamma']),\n",
    "                           actor.get_num_trainable_vars(),critic_net_params)\n",
    "\n",
    "\n",
    "    actor_noise = ActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "    if args['use_gym_monitor']:\n",
    "        if not args['render_env']:\n",
    "            env = wrappers.Monitor(\n",
    "                env, args['monitor_dir'], video_callable=False, force=True)\n",
    "        else:\n",
    "            env = wrappers.Monitor(env, args['monitor_dir'], force=True)\n",
    "\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    pretrain(session, env, args, actor, summary_ops, summary_vars)\n",
    "    train(session, env, args, actor, critic, actor_noise, summary_ops, summary_vars)\n",
    "\n",
    "    if args['use_gym_monitor']:\n",
    "        env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
